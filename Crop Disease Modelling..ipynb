{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2q27gKz1H20"
   },
   "source": [
    "##### Copyright 2021 The TensorFlow Authors.\n",
    "\n",
    "This notebook is adopted from the Tutorial by the TensorFlow authors and modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vvAObmTqglq"
   },
   "source": [
    "### Install the required packages\n",
    "Start by installing the required packages, including the Model Maker package from the [GitHub repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) and the pycocotools library you'll use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2Kwuc1RD4K8",
    "outputId": "6faf368a-ac00-4374-dd7a-8550b62a7aef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading Dataset Version Zip in PlantDoc-8 to voc: 100% [180587402 / 180587402] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to PlantDoc-8 in voc:: 100%|██████████| 8222/8222 [00:02<00:00, 3829.20it/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\" \") # I've hidden my API key\n",
    "project = rf.workspace().project(\"plantdoc-ljtfv\")\n",
    "dataset = project.version(8).download(\"voc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9H32tO_Gvho"
   },
   "outputs": [],
   "source": [
    "exit() #restart the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhl8lqVamEty",
    "outputId": "bd4bbb04-92ee-4e75-c0f0-f37533065ed9"
   },
   "outputs": [],
   "source": [
    "!pip install -q tflite-model-maker==0.3.4\n",
    "#!pip install -q tflite-support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6lRhVK9Q_0U"
   },
   "source": [
    "Import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "XtxiUeZEiXpt",
    "outputId": "e3d70c35-c165-413c-c2d6-33afd9dd0b41"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xushUyZXqP59"
   },
   "source": [
    "There are six steps to training an object detection model:\n",
    "\n",
    "**Step 1. Choose an object detection model archiecture.**\n",
    "\n",
    "This tutorial uses the EfficientDet-Lite2 model. EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture. \n",
    "\n",
    "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
    "\n",
    "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
    "|--------------------|-----------|---------------|----------------------|\n",
    "| EfficientDet-Lite0 | 4.4       | 37            | 25.69%               |\n",
    "| EfficientDet-Lite1 | 5.8       | 49            | 30.55%               |\n",
    "| EfficientDet-Lite2 | 7.2       | 69            | 33.97%               |\n",
    "| EfficientDet-Lite3 | 11.4      | 116           | 37.70%               |\n",
    "| EfficientDet-Lite4 | 19.9      | 260           | 41.96%               |\n",
    "\n",
    "<i> * Size of the integer quantized models. <br/>\n",
    "** Latency measured on Pixel 4 using 4 threads on CPU. <br/>\n",
    "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
    "</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtdZ-JDwMimd"
   },
   "outputs": [],
   "source": [
    "spec = model_spec.get('efficientdet_lite4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5U-A3tw6Y27"
   },
   "source": [
    "**Step 2. Load the dataset.**\n",
    "\n",
    "Model Maker will take input data in the CSV format. Use the `ObjectDetectorDataloader.from_csv` method to load the dataset and split them into the training, validation and test images.\n",
    "\n",
    "* Training images: These images are used to train the object detection model to recognize salad ingredients.\n",
    "* Validation images: These are images that the model didn't see during the training process. You'll use them to decide when you should stop the training, to avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
    "* Test images: These images are used to evaluate the final model performance.\n",
    "\n",
    "You can load the CSV file directly from Google Cloud Storage, but you don't need to keep your images on Google Cloud to use Model Maker. You can specify a local CSV file on your computer, and Model Maker will work just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKJsK0Lz3TjX"
   },
   "outputs": [],
   "source": [
    "main_folder = \"/content/PlantDoc-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7zxYRlGl_Mx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "def get_list(folders):\n",
    "  path = main_folder + \"/\"+ folders\n",
    "  extension = 'xml'\n",
    "  os.chdir(path)\n",
    "  result = glob.glob('*.{}'.format(extension))\n",
    "  result = [i[:-4] for i in result]\n",
    "  return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2u6I1lDWTjf4"
   },
   "outputs": [],
   "source": [
    "#https://gist.github.com/rotemtam/88d9a4efae243fc77ed4a0f9917c8f6c\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def xml_to_csv(path):\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            bbx = member.find('bndbox')\n",
    "            xmin = int(bbx.find('xmin').text)\n",
    "            ymin = int(bbx.find('ymin').text)\n",
    "            xmax = int(bbx.find('xmax').text)\n",
    "            ymax = int(bbx.find('ymax').text)\n",
    "            label = member.find('name').text\n",
    "\n",
    "            value = (root.find('filename').text,\n",
    "                     int(root.find('size')[0].text),\n",
    "                     int(root.find('size')[1].text),\n",
    "                     label,\n",
    "                     xmin,\n",
    "                     ymin,\n",
    "                     xmax,\n",
    "                     ymax\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['filename', 'width', 'height',\n",
    "                   'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37WtWeMqTlUp"
   },
   "outputs": [],
   "source": [
    "mapping = xml_to_csv(main_folder + \"/train\")[\"class\"].unique().tolist()\n",
    "print(mapping)\n",
    "print(len(mapping), \"classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZu-xghEny6c"
   },
   "outputs": [],
   "source": [
    "get_list(\"train/\")[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtEm6Denl-Yv"
   },
   "outputs": [],
   "source": [
    "train_data = object_detector.DataLoader.from_pascal_voc(main_folder + \"/train\", annotations_dir =main_folder + \"/train\" ,annotation_filenames=get_list(\"train\"), label_map=mapping)\n",
    "validation_data = object_detector.DataLoader.from_pascal_voc(main_folder + \"/valid\", annotations_dir =main_folder + \"/valid\", annotation_filenames=get_list(\"valid\"), label_map=mapping)\n",
    "test_data = object_detector.DataLoader.from_pascal_voc(main_folder + \"/test\", annotations_dir = main_folder + \"/test\",annotation_filenames= get_list(\"test\"), label_map=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Neafk9qOHLrZ"
   },
   "outputs": [],
   "source": [
    "test_data = object_detector.DataLoader.from_pascal_voc(main_folder + \"/test\", annotations_dir = main_folder + \"/test\",annotation_filenames= get_list(\"test\")[:30], label_map=mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uZkLR6N6gDR"
   },
   "source": [
    "**Step 3. Train the TensorFlow model with the training data.**\n",
    "\n",
    "* The EfficientDet-Lite0 model uses `epochs = 50` by default, which means it will go through the training dataset 50 times. You can look at the validation accuracy during training and stop early to avoid overfitting.\n",
    "* Set `batch_size = 8` here so you will see that it takes 21 steps to go through the 175 images in the training dataset. \n",
    "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwlYdTcg63xy",
    "outputId": "c06cef92-2009-4160-a632-b663891028f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "427/427 [==============================] - 634s 1s/step - det_loss: 1.4724 - cls_loss: 1.1711 - box_loss: 0.0060 - reg_l2_loss: 0.1129 - loss: 1.5853 - learning_rate: 0.0090 - gradient_norm: 2.7179 - val_det_loss: 1.0868 - val_cls_loss: 0.8980 - val_box_loss: 0.0038 - val_reg_l2_loss: 0.1132 - val_loss: 1.1999\n",
      "Epoch 2/40\n",
      "427/427 [==============================] - 572s 1s/step - det_loss: 0.9807 - cls_loss: 0.7798 - box_loss: 0.0040 - reg_l2_loss: 0.1134 - loss: 1.0941 - learning_rate: 0.0100 - gradient_norm: 2.9909 - val_det_loss: 0.8820 - val_cls_loss: 0.7375 - val_box_loss: 0.0029 - val_reg_l2_loss: 0.1136 - val_loss: 0.9957\n",
      "Epoch 3/40\n",
      "427/427 [==============================] - 572s 1s/step - det_loss: 0.8779 - cls_loss: 0.6916 - box_loss: 0.0037 - reg_l2_loss: 0.1138 - loss: 0.9917 - learning_rate: 0.0099 - gradient_norm: 3.0147 - val_det_loss: 0.8878 - val_cls_loss: 0.7697 - val_box_loss: 0.0024 - val_reg_l2_loss: 0.1141 - val_loss: 1.0019\n",
      "Epoch 4/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.8288 - cls_loss: 0.6515 - box_loss: 0.0035 - reg_l2_loss: 0.1143 - loss: 0.9430 - learning_rate: 0.0098 - gradient_norm: 3.0589 - val_det_loss: 0.8548 - val_cls_loss: 0.6889 - val_box_loss: 0.0033 - val_reg_l2_loss: 0.1145 - val_loss: 0.9693\n",
      "Epoch 5/40\n",
      "427/427 [==============================] - 628s 1s/step - det_loss: 0.7985 - cls_loss: 0.6246 - box_loss: 0.0035 - reg_l2_loss: 0.1147 - loss: 0.9132 - learning_rate: 0.0097 - gradient_norm: 3.0090 - val_det_loss: 0.7090 - val_cls_loss: 0.5682 - val_box_loss: 0.0028 - val_reg_l2_loss: 0.1148 - val_loss: 0.8239\n",
      "Epoch 6/40\n",
      "427/427 [==============================] - 572s 1s/step - det_loss: 0.7713 - cls_loss: 0.6036 - box_loss: 0.0034 - reg_l2_loss: 0.1150 - loss: 0.8863 - learning_rate: 0.0095 - gradient_norm: 3.0164 - val_det_loss: 0.7469 - val_cls_loss: 0.6202 - val_box_loss: 0.0025 - val_reg_l2_loss: 0.1152 - val_loss: 0.8622\n",
      "Epoch 7/40\n",
      "427/427 [==============================] - 572s 1s/step - det_loss: 0.7536 - cls_loss: 0.5887 - box_loss: 0.0033 - reg_l2_loss: 0.1154 - loss: 0.8690 - learning_rate: 0.0093 - gradient_norm: 3.1278 - val_det_loss: 0.6331 - val_cls_loss: 0.5119 - val_box_loss: 0.0024 - val_reg_l2_loss: 0.1156 - val_loss: 0.7487\n",
      "Epoch 8/40\n",
      "427/427 [==============================] - 572s 1s/step - det_loss: 0.7272 - cls_loss: 0.5672 - box_loss: 0.0032 - reg_l2_loss: 0.1158 - loss: 0.8430 - learning_rate: 0.0091 - gradient_norm: 3.0770 - val_det_loss: 0.7583 - val_cls_loss: 0.6266 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.1159 - val_loss: 0.8743\n",
      "Epoch 9/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.7069 - cls_loss: 0.5501 - box_loss: 0.0031 - reg_l2_loss: 0.1161 - loss: 0.8230 - learning_rate: 0.0089 - gradient_norm: 3.0796 - val_det_loss: 0.6897 - val_cls_loss: 0.5669 - val_box_loss: 0.0025 - val_reg_l2_loss: 0.1163 - val_loss: 0.8060\n",
      "Epoch 10/40\n",
      "427/427 [==============================] - 618s 1s/step - det_loss: 0.7022 - cls_loss: 0.5455 - box_loss: 0.0031 - reg_l2_loss: 0.1165 - loss: 0.8187 - learning_rate: 0.0086 - gradient_norm: 3.1746 - val_det_loss: 0.6112 - val_cls_loss: 0.4942 - val_box_loss: 0.0023 - val_reg_l2_loss: 0.1166 - val_loss: 0.7278\n",
      "Epoch 11/40\n",
      "427/427 [==============================] - 572s 1s/step - det_loss: 0.6709 - cls_loss: 0.5198 - box_loss: 0.0030 - reg_l2_loss: 0.1167 - loss: 0.7877 - learning_rate: 0.0083 - gradient_norm: 3.0016 - val_det_loss: 0.7587 - val_cls_loss: 0.6199 - val_box_loss: 0.0028 - val_reg_l2_loss: 0.1168 - val_loss: 0.8756\n",
      "Epoch 12/40\n",
      "427/427 [==============================] - 572s 1s/step - det_loss: 0.6664 - cls_loss: 0.5186 - box_loss: 0.0030 - reg_l2_loss: 0.1170 - loss: 0.7835 - learning_rate: 0.0080 - gradient_norm: 3.3126 - val_det_loss: 0.6870 - val_cls_loss: 0.5455 - val_box_loss: 0.0028 - val_reg_l2_loss: 0.1172 - val_loss: 0.8042\n",
      "Epoch 13/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.6547 - cls_loss: 0.5074 - box_loss: 0.0029 - reg_l2_loss: 0.1173 - loss: 0.7720 - learning_rate: 0.0077 - gradient_norm: 3.2296 - val_det_loss: 0.7073 - val_cls_loss: 0.5617 - val_box_loss: 0.0029 - val_reg_l2_loss: 0.1174 - val_loss: 0.8248\n",
      "Epoch 14/40\n",
      "427/427 [==============================] - 572s 1s/step - det_loss: 0.6437 - cls_loss: 0.5006 - box_loss: 0.0029 - reg_l2_loss: 0.1176 - loss: 0.7612 - learning_rate: 0.0073 - gradient_norm: 3.2794 - val_det_loss: 0.5992 - val_cls_loss: 0.4811 - val_box_loss: 0.0024 - val_reg_l2_loss: 0.1177 - val_loss: 0.7169\n",
      "Epoch 15/40\n",
      "427/427 [==============================] - 616s 1s/step - det_loss: 0.6192 - cls_loss: 0.4788 - box_loss: 0.0028 - reg_l2_loss: 0.1178 - loss: 0.7370 - learning_rate: 0.0070 - gradient_norm: 3.2077 - val_det_loss: 0.5986 - val_cls_loss: 0.4732 - val_box_loss: 0.0025 - val_reg_l2_loss: 0.1179 - val_loss: 0.7165\n",
      "Epoch 16/40\n",
      "427/427 [==============================] - 572s 1s/step - det_loss: 0.6063 - cls_loss: 0.4673 - box_loss: 0.0028 - reg_l2_loss: 0.1180 - loss: 0.7243 - learning_rate: 0.0066 - gradient_norm: 3.2961 - val_det_loss: 0.6394 - val_cls_loss: 0.5317 - val_box_loss: 0.0022 - val_reg_l2_loss: 0.1180 - val_loss: 0.7575\n",
      "Epoch 17/40\n",
      "427/427 [==============================] - 572s 1s/step - det_loss: 0.5933 - cls_loss: 0.4579 - box_loss: 0.0027 - reg_l2_loss: 0.1181 - loss: 0.7114 - learning_rate: 0.0062 - gradient_norm: 3.3165 - val_det_loss: 0.7669 - val_cls_loss: 0.6391 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.1182 - val_loss: 0.8851\n",
      "Epoch 18/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.5748 - cls_loss: 0.4436 - box_loss: 0.0026 - reg_l2_loss: 0.1182 - loss: 0.6930 - learning_rate: 0.0058 - gradient_norm: 3.2837 - val_det_loss: 0.5849 - val_cls_loss: 0.4680 - val_box_loss: 0.0023 - val_reg_l2_loss: 0.1183 - val_loss: 0.7032\n",
      "Epoch 19/40\n",
      "427/427 [==============================] - 572s 1s/step - det_loss: 0.5697 - cls_loss: 0.4379 - box_loss: 0.0026 - reg_l2_loss: 0.1184 - loss: 0.6881 - learning_rate: 0.0054 - gradient_norm: 3.5306 - val_det_loss: 0.6024 - val_cls_loss: 0.4774 - val_box_loss: 0.0025 - val_reg_l2_loss: 0.1184 - val_loss: 0.7209\n",
      "Epoch 20/40\n",
      "427/427 [==============================] - 617s 1s/step - det_loss: 0.5606 - cls_loss: 0.4296 - box_loss: 0.0026 - reg_l2_loss: 0.1185 - loss: 0.6791 - learning_rate: 0.0050 - gradient_norm: 3.3783 - val_det_loss: 0.6623 - val_cls_loss: 0.5468 - val_box_loss: 0.0023 - val_reg_l2_loss: 0.1185 - val_loss: 0.7808\n",
      "Epoch 21/40\n",
      "427/427 [==============================] - 572s 1s/step - det_loss: 0.5465 - cls_loss: 0.4176 - box_loss: 0.0026 - reg_l2_loss: 0.1186 - loss: 0.6651 - learning_rate: 0.0046 - gradient_norm: 3.4610 - val_det_loss: 0.5753 - val_cls_loss: 0.4578 - val_box_loss: 0.0023 - val_reg_l2_loss: 0.1186 - val_loss: 0.6939\n",
      "Epoch 22/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.5304 - cls_loss: 0.4040 - box_loss: 0.0025 - reg_l2_loss: 0.1186 - loss: 0.6491 - learning_rate: 0.0042 - gradient_norm: 3.5035 - val_det_loss: 0.5689 - val_cls_loss: 0.4432 - val_box_loss: 0.0025 - val_reg_l2_loss: 0.1186 - val_loss: 0.6875\n",
      "Epoch 23/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.5210 - cls_loss: 0.3966 - box_loss: 0.0025 - reg_l2_loss: 0.1186 - loss: 0.6397 - learning_rate: 0.0038 - gradient_norm: 3.4819 - val_det_loss: 0.6995 - val_cls_loss: 0.5636 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.1187 - val_loss: 0.8181\n",
      "Epoch 24/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.5033 - cls_loss: 0.3850 - box_loss: 0.0024 - reg_l2_loss: 0.1187 - loss: 0.6219 - learning_rate: 0.0034 - gradient_norm: 3.4786 - val_det_loss: 0.6691 - val_cls_loss: 0.5541 - val_box_loss: 0.0023 - val_reg_l2_loss: 0.1187 - val_loss: 0.7878\n",
      "Epoch 25/40\n",
      "427/427 [==============================] - 617s 1s/step - det_loss: 0.4968 - cls_loss: 0.3763 - box_loss: 0.0024 - reg_l2_loss: 0.1187 - loss: 0.6154 - learning_rate: 0.0030 - gradient_norm: 3.4535 - val_det_loss: 0.6420 - val_cls_loss: 0.5179 - val_box_loss: 0.0025 - val_reg_l2_loss: 0.1187 - val_loss: 0.7607\n",
      "Epoch 26/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.4890 - cls_loss: 0.3715 - box_loss: 0.0024 - reg_l2_loss: 0.1187 - loss: 0.6077 - learning_rate: 0.0027 - gradient_norm: 3.5349 - val_det_loss: 0.6004 - val_cls_loss: 0.4693 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.1186 - val_loss: 0.7191\n",
      "Epoch 27/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.4794 - cls_loss: 0.3606 - box_loss: 0.0024 - reg_l2_loss: 0.1186 - loss: 0.5981 - learning_rate: 0.0023 - gradient_norm: 3.4263 - val_det_loss: 0.5759 - val_cls_loss: 0.4413 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.1186 - val_loss: 0.6945\n",
      "Epoch 28/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.4678 - cls_loss: 0.3532 - box_loss: 0.0023 - reg_l2_loss: 0.1186 - loss: 0.5865 - learning_rate: 0.0020 - gradient_norm: 3.4378 - val_det_loss: 0.6101 - val_cls_loss: 0.4839 - val_box_loss: 0.0025 - val_reg_l2_loss: 0.1186 - val_loss: 0.7287\n",
      "Epoch 29/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.4563 - cls_loss: 0.3443 - box_loss: 0.0022 - reg_l2_loss: 0.1186 - loss: 0.5748 - learning_rate: 0.0017 - gradient_norm: 3.5282 - val_det_loss: 0.6323 - val_cls_loss: 0.5015 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.1186 - val_loss: 0.7509\n",
      "Epoch 30/40\n",
      "427/427 [==============================] - 616s 1s/step - det_loss: 0.4532 - cls_loss: 0.3407 - box_loss: 0.0022 - reg_l2_loss: 0.1186 - loss: 0.5718 - learning_rate: 0.0014 - gradient_norm: 3.4869 - val_det_loss: 0.6152 - val_cls_loss: 0.4878 - val_box_loss: 0.0025 - val_reg_l2_loss: 0.1185 - val_loss: 0.7338\n",
      "Epoch 31/40\n",
      "427/427 [==============================] - 574s 1s/step - det_loss: 0.4481 - cls_loss: 0.3358 - box_loss: 0.0022 - reg_l2_loss: 0.1185 - loss: 0.5666 - learning_rate: 0.0011 - gradient_norm: 3.4657 - val_det_loss: 0.5739 - val_cls_loss: 0.4406 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.1185 - val_loss: 0.6924\n",
      "Epoch 32/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.4413 - cls_loss: 0.3309 - box_loss: 0.0022 - reg_l2_loss: 0.1185 - loss: 0.5598 - learning_rate: 8.8620e-04 - gradient_norm: 3.4233 - val_det_loss: 0.5670 - val_cls_loss: 0.4352 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.1185 - val_loss: 0.6855\n",
      "Epoch 33/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.4379 - cls_loss: 0.3260 - box_loss: 0.0022 - reg_l2_loss: 0.1185 - loss: 0.5563 - learning_rate: 6.7105e-04 - gradient_norm: 3.4580 - val_det_loss: 0.5734 - val_cls_loss: 0.4397 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.1185 - val_loss: 0.6919\n",
      "Epoch 34/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.4280 - cls_loss: 0.3201 - box_loss: 0.0022 - reg_l2_loss: 0.1185 - loss: 0.5465 - learning_rate: 4.8398e-04 - gradient_norm: 3.3416 - val_det_loss: 0.5508 - val_cls_loss: 0.4225 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.1185 - val_loss: 0.6693\n",
      "Epoch 35/40\n",
      "427/427 [==============================] - 616s 1s/step - det_loss: 0.4306 - cls_loss: 0.3208 - box_loss: 0.0022 - reg_l2_loss: 0.1185 - loss: 0.5490 - learning_rate: 3.2619e-04 - gradient_norm: 3.3773 - val_det_loss: 0.5387 - val_cls_loss: 0.4108 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.1185 - val_loss: 0.6572\n",
      "Epoch 36/40\n",
      "427/427 [==============================] - 572s 1s/step - det_loss: 0.4284 - cls_loss: 0.3193 - box_loss: 0.0022 - reg_l2_loss: 0.1185 - loss: 0.5469 - learning_rate: 1.9871e-04 - gradient_norm: 3.4302 - val_det_loss: 0.5508 - val_cls_loss: 0.4200 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.1185 - val_loss: 0.6693\n",
      "Epoch 37/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.4278 - cls_loss: 0.3192 - box_loss: 0.0022 - reg_l2_loss: 0.1185 - loss: 0.5462 - learning_rate: 1.0238e-04 - gradient_norm: 3.3550 - val_det_loss: 0.5578 - val_cls_loss: 0.4265 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.1185 - val_loss: 0.6762\n",
      "Epoch 38/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.4192 - cls_loss: 0.3128 - box_loss: 0.0021 - reg_l2_loss: 0.1184 - loss: 0.5376 - learning_rate: 3.7804e-05 - gradient_norm: 3.3664 - val_det_loss: 0.5488 - val_cls_loss: 0.4180 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.1184 - val_loss: 0.6673\n",
      "Epoch 39/40\n",
      "427/427 [==============================] - 573s 1s/step - det_loss: 0.4183 - cls_loss: 0.3113 - box_loss: 0.0021 - reg_l2_loss: 0.1184 - loss: 0.5367 - learning_rate: 5.4120e-06 - gradient_norm: 3.3010 - val_det_loss: 0.5515 - val_cls_loss: 0.4200 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.1184 - val_loss: 0.6699\n",
      "Epoch 40/40\n",
      "427/427 [==============================] - 616s 1s/step - det_loss: 0.4215 - cls_loss: 0.3141 - box_loss: 0.0021 - reg_l2_loss: 0.1184 - loss: 0.5399 - learning_rate: 5.4118e-06 - gradient_norm: 3.3053 - val_det_loss: 0.5561 - val_cls_loss: 0.4252 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.1184 - val_loss: 0.6745\n"
     ]
    }
   ],
   "source": [
    "model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data, epochs = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BzCHLWJ6h7q"
   },
   "source": [
    "**Step 4. Evaluate the model with the test data.**\n",
    "\n",
    "After training the object detection model using the images in the training dataset, use the remaining 25 images in the test dataset to evaluate how the model performs against new data it has never seen before.\n",
    "\n",
    "As the default batch size is 64, it will take 1 step to go through the images in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xmnl6Yy7ARn"
   },
   "outputs": [],
   "source": [
    "#model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgCDMe0e6jlT"
   },
   "source": [
    "**Step 5.  Export as a TensorFlow Lite model.**\n",
    "\n",
    "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is full integer quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hm_UULdW7A9T"
   },
   "outputs": [],
   "source": [
    "model.export(export_dir='/content/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UygYErfCD5m3"
   },
   "source": [
    "**Step 6.  Evaluate the TensorFlow Lite model.**\n",
    "\n",
    "Several factors can affect the model accuracy when exporting to TFLite:\n",
    "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop. \n",
    "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
    "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
    "\n",
    "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHYDWcljr6jq",
    "outputId": "878f9a51-30f2-47e2-8ba3-bb6e049381fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 1450s 48s/step\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AP': 0.56244385,\n",
       " 'AP50': 0.69599766,\n",
       " 'AP75': 0.67652184,\n",
       " 'AP_/Apple Scab Leaf': 0.7,\n",
       " 'AP_/Apple leaf': 0.8,\n",
       " 'AP_/Apple rust leaf': 0.7252475,\n",
       " 'AP_/Bell_pepper leaf': 0.44125414,\n",
       " 'AP_/Bell_pepper leaf spot': -1.0,\n",
       " 'AP_/Blueberry leaf': 0.9,\n",
       " 'AP_/Cherry leaf': 0.56336635,\n",
       " 'AP_/Corn Gray leaf spot': -1.0,\n",
       " 'AP_/Corn leaf blight': 0.9,\n",
       " 'AP_/Corn rust leaf': -1.0,\n",
       " 'AP_/Peach leaf': 0.83366334,\n",
       " 'AP_/Potato leaf': -1.0,\n",
       " 'AP_/Potato leaf early blight': 0.2539109,\n",
       " 'AP_/Potato leaf late blight': 0.0,\n",
       " 'AP_/Raspberry leaf': 0.8582508,\n",
       " 'AP_/Soyabean leaf': -1.0,\n",
       " 'AP_/Soybean leaf': -1.0,\n",
       " 'AP_/Squash Powdery mildew leaf': 0.0,\n",
       " 'AP_/Strawberry leaf': 0.8442244,\n",
       " 'AP_/Tomato Early blight leaf': -1.0,\n",
       " 'AP_/Tomato Septoria leaf spot': -1.0,\n",
       " 'AP_/Tomato leaf': -1.0,\n",
       " 'AP_/Tomato leaf bacterial spot': -1.0,\n",
       " 'AP_/Tomato leaf late blight': 0.7866337,\n",
       " 'AP_/Tomato leaf mosaic virus': -1.0,\n",
       " 'AP_/Tomato leaf yellow virus': 0.19219038,\n",
       " 'AP_/Tomato mold leaf': 0.0,\n",
       " 'AP_/Tomato two spotted spider mites leaf': -1.0,\n",
       " 'AP_/grape leaf': 0.5,\n",
       " 'AP_/grape leaf black rot': 0.8252475,\n",
       " 'APl': 0.58395386,\n",
       " 'APm': 0.27798602,\n",
       " 'APs': -1.0,\n",
       " 'ARl': 0.63954246,\n",
       " 'ARm': 0.43333334,\n",
       " 'ARmax1': 0.4331313,\n",
       " 'ARmax10': 0.6198653,\n",
       " 'ARmax100': 0.62651515,\n",
       " 'ARs': -1.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_tflite('/content/model.tflite', test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVxaf3x_7OfB"
   },
   "source": [
    "You can download the TensorFlow Lite model file using the left sidebar of Colab. Right-click the `model.tflite` file and choose `Download` to download it to your local computer.\n",
    "\n",
    "In the next step of the codelab, you'll use the [ObjectDetector API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector) of the [TensorFlow Lite Task Library](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview) to integrate the model into the Android app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cllbCFwQv6g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
